{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy  as np \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# We will use one-hot encoding to encode the categorical features. They will be as follows:\n",
    "\n",
    "Ear Shape: Pointy = 1, Floppy = 0\n",
    "Face Shape: Round = 1, Not Round = 0\n",
    "Whiskers: Present = 1, Absent = 0\n",
    "Therefore, we have two sets:\n",
    "\n",
    "X_train: for each example, contains 3 features:\n",
    "\n",
    "      - Ear Shape (1 if pointy, 0 otherwise)\n",
    "      - Face Shape (1 if round, 0 otherwise)\n",
    "      - Whiskers (1 if present, 0 otherwise)\n",
    "y_train: whether the animal is a cat\n",
    "\n",
    "      - 1 if the animal is a cat\n",
    "      - 0 otherwise\n",
    "check the image uploaded for further info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 1, 1],\n",
    "[0, 0, 1],\n",
    " [0, 1, 0],\n",
    " [1, 0, 1],\n",
    " [1, 1, 1],\n",
    " [1, 1, 0],\n",
    " [0, 0, 0],\n",
    " [1, 1, 0],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0]])\n",
    "\n",
    "y_train = np.array([1, 1, 0, 0, 1, 1, 0, 1, 0, 0])\n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On each node, we compute the information gain for each feature, then split the node on the feature with the higher information gain, by comparing the entropy of the node with the weighted entropy in the two splitted nodes.\n",
    "\n",
    "So, the root node has every animal in our dataset. Remember that 𝑝𝑛𝑜𝑑𝑒1\n",
    " is the proportion of positive class (cats) in the root node. So\n",
    "\n",
    "𝑝𝑛𝑜𝑑𝑒1=510=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def entropy(p):\n",
    "    if p==0 or p==1:\n",
    "        return 0\n",
    "\n",
    "    else:\n",
    "        return -p * np.log2(p) - (1-p) * np.log2(1-p)\n",
    "\n",
    "print(entropy(0.5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate, let's compute the information gain if we split the node for each of the features. To do this, let's write some functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_indices(X, index_feature):\n",
    "    \"\"\"Given a dataset and a index feature, return two lists for the two split nodes, the left node has the animals that have \n",
    "    that feature = 1 and the right node those that have the feature = 0 \n",
    "    index feature = 0 => ear shape\n",
    "    index feature = 1 => face shape\n",
    "    index feature = 2 => whiskers\n",
    "    \"\"\"\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    for i,x in enumerate (X):\n",
    "        if(x[index_feature] == 1):\n",
    "            left_indices.append(i)\n",
    "\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "    return left_indices, right_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 3, 4, 5, 7], [1, 2, 6, 8, 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_indices(X_train, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need another function to compute the weighted entropy in the splitted nodes. As you've seen in the video lecture, we must find:\n",
    "\n",
    "𝑤left\n",
    "  and  𝑤right\n",
    " , the proportion of animals in each node.\n",
    "𝑝left\n",
    "  and  𝑝right\n",
    " , the proportion of cats in each split.\n",
    "Note the difference between these two definitions!! To illustrate, if we split the root node on the feature of index 0 (Ear Shape), then in the left node, the one that has the animals 0, 3, 4, 5 and 7, we have:\n",
    "\n",
    "𝑤left=5/10=0.5 and 𝑝left=4/5\n",
    " \n",
    "𝑤right=5/10=0.5 and 𝑝right=1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(X, y, left_indices, right_indices):\n",
    "    #w-left = no. of nodes in leftmost leaf nodes/total nodes\n",
    "    w_left = len(left_indices)/len(X)\n",
    "    w_right = len(right_indices)/len(X)\n",
    "    #enntropy = fraction of training examples that are cats\n",
    "    p_left = sum(y[left_indices])/len(left_indices)\n",
    "    p_right = sum(y[right_indices])/len(right_indices)\n",
    "\n",
    "    weighted_entropy = entropy(p_left) * w_left + entropy(p_right) * w_right\n",
    "    return weighted_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219280948873623"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left_indices, right_indices = split_indices(X_train, 0)\n",
    "weighted_entropy(X_train, y_train, left_indices, right_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate information gain: \n",
    "\n",
    "In this code, p_node refers to the proportion of positive examples (or the probability of a positive example) in the parent node. It is calculated by dividing the total number of positive examples by the total number of examples in the node:\n",
    "\n",
    "p_node = sum(y)/len(y)\n",
    "\n",
    "where y is the list of class labels for the examples in the node, and sum(y) returns the total number of positive examples (since positive examples are represented as 1 and negative examples as 0).\n",
    "\n",
    "This value is used to calculate the entropy of the parent node, which is a measure of the impurity of the class distribution before the split.\n",
    "\n",
    "Information gain is a measure of the difference between the entropy of the parent node and the weighted average of the entropies of the child nodes after a split in a decision tree. The formula for information gain can be written as:\n",
    "\n",
    "IG = entropy(parent) - weighted_entropy(children)\n",
    "\n",
    "A higher information gain indicates a better split, as it results in more pure or homogeneous subsets of the data.\n",
    "\n",
    "If IG = 0, do not split the tree furhter.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(X, y, left_indices, right_indices):\n",
    "    p_node = sum(y)/len(y)\n",
    "    h_node = entropy(p_node)\n",
    "    w_entropy = weighted_entropy(X, y, left_indices, right_indices)\n",
    "    return h_node - w_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2780719051126377"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain(X_train, y_train, left_indices, right_indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the information gain if we split the root node for each feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: Ear Shape, information gain if we split the root node using this feature: 0.28\n",
      "Feature: Face Shape, information gain if we split the root node using this feature: 0.03\n",
      "Feature: Whiskers, information gain if we split the root node using this feature: 0.12\n"
     ]
    }
   ],
   "source": [
    "for i, feature_name in enumerate (['Ear Shape', 'Face Shape', 'Whiskers']):\n",
    "    left_indices, right_indices = split_indices(X_train, i)\n",
    "    i_gain = information_gain(X_train,y_train, left_indices, right_indices)\n",
    "    print(f\"Feature: {feature_name}, information gain if we split the root node using this feature: {i_gain:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionNode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tree \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m build_tree_recursive(X_train, y_train, np\u001b[39m.\u001b[39;49marray([\u001b[39m0\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m4\u001b[39;49m,\u001b[39m5\u001b[39;49m,\u001b[39m6\u001b[39;49m,\u001b[39m7\u001b[39;49m,\u001b[39m8\u001b[39;49m,\u001b[39m9\u001b[39;49m]), \u001b[39m\"\u001b[39;49m\u001b[39mRoot\u001b[39;49m\u001b[39m\"\u001b[39;49m, max_depth\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, current_depth\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, tree\u001b[39m=\u001b[39;49mtree)\n\u001b[0;32m      3\u001b[0m generate_tree_viz([\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m6\u001b[39m,\u001b[39m7\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m9\u001b[39m], y_train, tree)\n",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m, in \u001b[0;36mbuild_tree_recursive\u001b[1;34m(X, y, indices, node_name, max_depth, current_depth, tree)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mif\u001b[39;00m current_depth \u001b[39m==\u001b[39m max_depth \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(indices) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      3\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m node \u001b[39m=\u001b[39m DecisionNode(node_name)\n\u001b[0;32m      5\u001b[0m tree\u001b[39m.\u001b[39mappend(node)\n\u001b[0;32m      6\u001b[0m node\u001b[39m.\u001b[39mindices \u001b[39m=\u001b[39m indices\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DecisionNode' is not defined"
     ]
    }
   ],
   "source": [
    "tree = []\n",
    "build_tree_recursive(X_train, y_train, [0,1,2,3,4,5,6,7,8,9], \"Root\", max_depth=2, current_depth=0, tree = tree)\n",
    "generate_tree_viz([0,1,2,3,4,5,6,7,8,9], y_train, tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
